---
title: "The Bird of Catastrophy"  
author: "Joel Anderson"  
date: "`r format(Sys.time(), '%B %d, %Y')`" 

output: bookdown::word_document2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, dpi = 600, fig.width = 4.5, fig.height = 3)
```

# Black Swans {.unnumbered}

The term "Black Swan" originates from the belief that all swans were white because those were the only ones observed by westerners. This belief continued until 1697 when black swans were found in Australia, rewriting accepted science in one swipe. The term becomes a metaphor for unprecedented and unexpected events and eventually the title of a best selling book by Nasim Taleb.

There are several characteristics that all Black Swans have in common. The first is that it comes as a surprise to the observer. It is an outlier outside the realm of regular expectations because nothing in the past points to its existence. Typically this is because the time frame of which data is available is too short to contain one of these Black Swans due to their rarity.

The second factor that all Black Swans have in common is that they are rationalized after the fact, making them "predictable" in retrospect in the observers mind. Explanations are created to convince the observer that if they had only looked, the evidence was there the whole time. Even though there is nothing in the past that points to it.

The third characteristic is that it has extreme impact beyond anything ever observed previously. The consequences can be thousands of times a "typical" incident. The three ingredients to a Black Swan are the unpredictability, consequences not seen in any historical record and rationalization after the fact.

# How Bad Can Bad Be {.unnumbered}

In any risk analysis the question always at hand is, how bad can it be? The response is reflexively, how bad has it been in the past? The problem with that approach is that Black Swans are a rare event and your window of observations might be too short to have experienced one and it can be easy to incorrectly equate lack of evidence with evidence of lack. If your entire data set contains nothing but ordinary "White Swans" it does nothing to prove the lack of existence of Black Swans but it only takes a single Black Swan to disprove that theory of its non-existence and put a company out of business in one swipe. Sage advice was given by Emil Gumbel when he said, "It is impossible for the improbable to never happen."

In addition, when people are asked about the magnitude of risk for something in which hard data is sparse or non-existent they will base their estimates based on personal experience which is most likely skewed toward the lower quartile. This is a consequence of normal common risk is easier to observe but it is not representative of extreme values that can occur. The equally bad practice is to base their estimate on, "from what they heard", since people are more likely to give credibility to stories than they are abstract statistics since it is easier to mentally process.

# Black Swans and Statistics {.unnumbered}

Most of the traditional statistics serve the user well if the phenomena of interest never ventures far from the average. It can be seen in the following examples that catastrophic risk does not adhere to such norms. Some distributions are so common in many everyday applications where the effects are additive and the deviations from the mean are small, it's refered to as a, "normal distribution". The classic example of this is height of people. The average height of men is about 5' 10" with a standard deviation of 4". It would be very rare to come across someone that is more than 2 or 3 standard deviations from that mean. We don't have giants or Lilliputians walking around!

But a Black Swan can be a hundred or a thousand times the mean. This is because the variables that drive this type of risk are multiplicative rather than additive therefore the results have the ability to increase exponentially. That is why it is very dangerous to make assumptions that make something computationally convenient (e.g. normally distributed or taking an average) because Black Swans live in the wild, out on the tails of the distribution where traditional statistics would predict the likelihood of something of that magnitude is essentially zero. The first characteristic of Black Swans is that they are a rare. The shorter the window of observations the smaller the probability of seeing even one. The "law" of small numbers which states that *there are not enough small numbers to satisfy all the demands placed on them*. This really isn't a law but a fallacy that occurs when a small sample is taken and people assume it's representative of the much larger population and they are surprised (usually negatively) later when a larger sample is taken.

# The Perils of the Normal Distribution {.unnumbered}

The results of a simulation are shown in Figure \@ref(fig:gumbel) which demonstrates why the normal distribution is misleading in the area of rare events. The simulation makes 1,000 random draws from a standard normal distribution (zero mean and unit standard deviation) and repeats this process 1,000 times (1 million samples total). With each iteration, it calculates the mean and maximum value, plotted as blue and red markers respectively, then plots the density curve for each set of random draws. The bottom plot shows that the probability density of the maximum values. The distribution of these values converge to a Gumbel distribution as the number of samples increases. This is also known as a Type I extreme value distribution which will be discussed later in this paper.

But more importantly, is that even though the samples are drawn at random from a normal distribution which is unlimited at either end, the average of the samples never get far from the mean (about $\pm$ 0.1 standard deviations) of the population and the extremes *start* at about 3 standard deviations from the mean. If the sample data were examined, it would be seen that only about one-tenth of one percent (0.1%) ever exceeded 3 standard deviations. The implications of this are that if an engineer is working with a set of limited observations and assumes the entire range of data is normally distributed, the magnitude and frequency of an extreme event will be grossly underestimated since the observation data would have a low probability of even containing anything beyond two standard deviations, much less three. But this also demonstrates that extreme events follow their own distribution separate from the rest of the population that can be exploited to the designer's benefit.

```{r libraries}
library(locfit)
library(logspline)
library(patchwork)
library(tinytex)
library(tidyverse)
library(extRemes)
# library(infer)
# theme_set(theme_bw(12,"serif"))

```

```{r gumbel, fig.cap = "Simulation Results", fig.height=3.25, fig.width=6.5, dpi=300, cache=TRUE}

set.seed(12345)
#Tidy verse way ---------

it <- 1:1000
n <- rep(0,max(it))

# df2 <- bind_cols(it = it)

df <- map(n, ~ rnorm(n = 1000, mean = .x, 1)) %>%
  tibble() %>%
  bind_cols(it =it) %>%
  unnest(cols = 1) %>%
  rename(value = ".")

max <- df %>%
  group_by(it) %>%
  summarise(max=max(value),
            ave=mean(value),
            .groups = "drop")

max(max$max)


norm_dist <- df %>%
  ggplot(aes(value)) +
  geom_density(col = 'grey',
               aes(group = it),
               alpha = 0.001, 
               lwd=0.15) +
  geom_point(data = max,
             aes(x = max, y = 0),
             col = 'red',
             shape = 2) +
  geom_point(data = max,
             aes(x = ave, y = 0),
             col = 'blue',
             shape = 2, alpha=0.1) +
  theme_minimal(15)+
  labs(x="Normal Distribution", y=NULL)


gumbel <- max %>%
  # filter(max>2.5) %>%
  ggplot(aes(max))+
  stat_density(geom="line",col='red',lwd=1)+
  theme_minimal(15)+
  coord_cartesian(xlim=c(-5,5))+
  labs(y =NULL,
       x = "Extreme Value Distribution (Gumbel)")

avg <- max %>%
  # filter(max>2.5) %>%
  ggplot(aes(ave))+
  stat_density(geom="line",col='blue',lwd=1)+
  theme_minimal(15)+
  xlim(-5, 5)+
  labs(y =NULL,
       x = "Avg. Distribution")

norm_dist/gumbel


```

It can be said that statistics is a statement about the future based on observations in the past. But risk is not a constant thing. People constantly alter their behavior based on observed or perceived dangers therefore altering future outcomes. Making past observations possibly moot if it based on an assumed pattern of behavior. Donald Rumsfeld, former Secretary of Defense talked about the three types of "knowns". The first is the "known knowns", things that you know for certain. The second is the "known unknowns", these are the things that you know you don't know but since you are aware of them they could be acquired through time, effort or other resources. The third and most consequential is the "unknown unknowns", these are the things that you don't know you don't know - you are completely unaware of them in any way. This is the origins of Black Swans. For if the factors that could lead to a Black Swan were known, the end user would have made adjustments to their behavior to account for them. But instead the average person careens ahead attempting to mitigate and prevent the directly perceived more common risk factors that are seen on a regular basis, in the meantime just out of sight could be an event that end their very existence. This is known as availability bias, the more recent events are given more credibility even if evidence to the contrary exists. An example would be people that they say they would never swim in the ocean after hearing of a shark attack, even though it is an extremely rare event when compared to things like being attacked by dogs. Sharks are certainly more deadly but you have far more encounters with dogs than sharks in your daily life making total the probability of being attacked by a canine is much greater than the sea-going predator. Yet the shark is perceived to be the bigger threat to one's life.

# You Make the Call {.unnumbered}

There used to be a commercial that played during NFL games where they would show footage of an unusual play from an NFL game and then ask the audience, "You make the call". In this exercise you are handed the following data seen in Figure \@ref(fig:insurance-loss) and are asked what is the likely worst case scenario for loss ratios for the next year. What would you say? Hopefully you know enough that the average is too optimistic and the worst data point on record is probably too pessimistic. You could assume the data is normally distributed and that 95% of the population lies approximately within two standard deviations either side of the mean. So you make your worst case prediction, of 67. After all, the entire 23 years of data is less than 50 with the exception of one outlier year and 75% of the data is 12 or less. Seems like a solid prediction to the engineer. But is it?

```{r insurance-loss, fig.cap= "Insurance Losses"}

eq_insurance <- read.csv("earthquake_loss.csv")

eq_insurance %>% 
  filter(year!=1994) %>%  
ggplot(aes(year, loss_ratio))+
  geom_bar(stat = "identity", 
           fill='steelblue2',
           col='black')+
  theme_bw(10, "serif")+
  scale_y_continuous(breaks=seq(0,140,by=20))+
  labs(title = "Earthquake Insurance Loss Ratios by Year",
       subtitle = "1971 - 1993", 
       x="Year", 
       y="Loss Ratio")
```

The conclusion of this exercise is that in 1994 the Northridge Earthquake hit with a loss ratio of 2273, almost 700 times the median loss ratio of the previous 23 years. Even if someone was to assume worst case was the mean plus three standard deviations, which under the normal distribution assumption would cover 99.7% of the data, they would have still been off by a factor of more than 20. What this example shows is that catastrophic risk does not follow statistical norms. In the field of risk, the consequences have the ability to increase exponentially and are not bound by the artificial constraints of a normal distribution.

Consequently, even analysis methods like a Monte Carlo, though they account for stochastic nature of different variables, are better at predicting average behavior of something than the extremes. This is because the sampling distributions used are more suited to modeling the "typical" rather than the extremes, leading to "typical" results.

# Catastrophic Risk {.unnumbered}

While history is not always a good predictor of the future, history is what is available to mine for information. In the case of pipeline risk, PHMSA publishes incident data for pipelines. An incident is an accidental release from a pipeline that includes any of the following: Greater than \$50,000 in property damage, an injury or fatality. If the reported property damage data is taken and the injuries and fatalities are scaled to dollar values using an appropriate multiplier, the sum of these produce a total consequence of failure (CoF). The CoF gives a common reference across all incidents of varying property damage, injuries and fatalities. The data from 1986 to 2020 is shown in a histogram in Figure \@ref(fig:incident-plot). Note: the range of data for the histogram extends out to $10^9$ on the logarithmic axis but for the purposes of scale it is truncated at $10^8$.

```{r importdata}

incidents <- read_csv("gt_all.csv")
  # rename(CoF = COF) %>% 
  # filter(!is.na(CoF))

#Max incidents per year
maxi <- incidents %>% 
  group_by(Year) %>% 
  summarise(maxi = max(CoF))

```

```{r incident-plot, fig.cap="CoF Data Histogram"}
incidents %>%
  filter(
    # str_detect(SYSTEM_TYPE, pattern = "GT"),
         between(CoF, 1e6, 1e8)) %>%
  ggplot(aes(CoF)) +
  geom_histogram(fill = '#FF3030',
                 col = 'black',
                 alpha = 0.8) +
  geom_rug(col = 'blue') +
  theme_bw(11, "serif") +
  # xlim(0, 1e8) +
  # ylim(0, 200) +
  labs(title = "Consequence of Failure",
       subtitle = "PHMSA Gas Transmission Incidents 1986 - 2020",
       x = "Consequence of Failure ($) - log Scale") +
  theme(plot.margin = margin(
    r = 0.7,
    t = 0.6,
    b = 0.5,
    l = 0.5,
    unit = "cm"
  ))

```

In the field of risk and in most meteorological phenomena such as floods and wind speeds, the majority of the data will almost always lie in a predictable range near the lower end of the scale. The tick marks beneath the incident histogram is a rug plot which places a single mark for each data point along the axis. Even though the incidents data is heavily concentrated to the left end of the scale, the incidents that should keep one up at night are the ones to the far right which are much rarer and the consequences much larger. In the CoF data in Figure \@ref(fig:incident-plot) the median incident is around \$170,000 and the maximum is over \$1 billion a difference of over 6,000 times. This is common in any application of risk, not just pipelines. There will be a plethora of data at the lower end of the scale with outliers extending out several magnitudes of order. But this concept is more than an academic exercise. The implications of this is seen in Figure \@ref(fig:incident-gt10M-plot) where 5% of all incidents make up 65% of the total consequences since 1986. Putting it another way, about 2 to 3 incidents a year account for about two-thirds of all consequences.

The costliest incident on record is well-known to be San Bruno but the second costliest is not Carlsbad as you would think. It was an incident in a Class I location in rural eastern Texas. Don't assume that because a pipeline is in a rural location that it is immune from catastrophic consequences.

```{r incident-gt10M-plot, fig.cap="Significant Incident Consequences", fig.width=6.5, fig.height=4}

incidents %>%
  filter(Significant == "YES" | Serious == "YES") %>%
  ggplot() +
  geom_histogram(aes(CoF, y = ..density..),
                 fill = 'firebrick2',
                 col = 'black') +
  labs(
    x = "Consequence of Failure ($) - log scale",
    title = "Significant Incident Consequences",
    subtitle = "PHMSA Incident Data (1986-2020)",
    y = NULL
  ) +
  geom_vline(xintercept = 10e6, lty = 2) +
  scale_x_log10() +
  theme_bw(11, "serif") +
  theme(
    plot.margin = margin(0.6, 0.6, 0.6, 0.6, "cm"),
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank()
  ) +
  annotate(
    x = 1.6e6,
    y = 0.8,
    geom = "text",
    label = "Incidents<$10MM:\n $2.1B\ Total CoF\n 95% of incidents = 35% of CoF",
    size = 3.5
  ) +
  annotate(
    x = 6.5e7,
    y = 0.8,
    geom = "text",
    label = "Incidents>$10MM:\n$3.6B Total CoF\n5% of incidents = 65% of CoF",
    size = 3.5
  )
```

In a data set such as the incident data, a distribution will often have the appearance of being a good fit to one of the standard distributions through the middle of the curve with a few outliers at the higher end giving a false sense of security that future data points are accounted for. But if the tails area was magnified, it would be seen that there are far more of these extreme events than would be expected based on the inferred probability. This is why most distributions are only a suitable model for such events where the phenomena is based on sums and averages but a really poor model for extreme events. If traditional statistics held up for extremes, 35 years of data would not be enough to experience anything like a San Bruno or even a Carlsbad yet there are several examples in the data departing from this assumed probability distribution. This implies that there is something else outside of traditional statistics that describes these extremes. In fact the data departs from the predicted count by a factor of 10 at the higher CoF values. The seemingly goodness of fit in the more prevalent lower end will often lead people to disregard the extreme events as so remote as not to be given serious consideration, much to their peril.

# Extremes {.unnumbered}

The aim of statistical theory of extreme values is to explain the observed extremes arising in the samples of given sizes, or for a given period and to forecast extremes that may be expected to occur within a certain sample size, time, area, etc. There are two types of predictions that are can be made, one is the intensity of an event and the other is the frequency. The frequency can be based on a set of observations or on a known period of return. Many incorrect assumptions about extremes have been carried forward for centuries, the erroneous conclusion that about three times the standard deviation should be considered as the maximum for any statistical variable has long been a "rule of thumb". The idea that this assumption should be considered as maximum-irrespective of the number of observations and of the distribution still prevails among most "practical" people. However, the fallacy of this rule is obvious. If the phenomena being studied is unlimited then the largest value is unlimited as well. Emil Gumbel was once quoted, "There will always be one (or more) value that will exceed all others."[^1] and the President's Water Commission (1950) stated that,"However big floods get, there will always be a bigger one coming; so says one theory of extremes, and experience suggests it is true." [^2]

[^1]: Les Valeurs Extremes des Distributions Statistiques', Annales de 'Institut Henri Poincar (1935)

[^2]: President's Water Commission, 1950, Vol. 1, p.141

Fortunately this phenomena has been studied at length in areas such as floods and insurance risks. The first problem of the extremes to be discussed is when the available data is set of observations over a fixed period of time and the desire is to know the probability of exceeding the maximum observed value in some amount of time in the future. An example of this would be 25 years of rainfall data is what is available and the engineer wants to know what the likelihood of exceeding the maximum observed amount in the next 50 years.

# The Law of Rare Exceedances {.unnumbered}

Often, the true frequency of an extreme event is not known a priori and the engineer only has a set of observations over a limited time period to work with. But the drawback to relying on solely observed data is that the shorter the observation period, the more likely the largest event observed will be exceeded in future observations and in a shorter amount of time. As the sample size is increased, the largest value observed will likewise increase. But with something that is a rare event it is impractical to wait for more data, therefore a model has to be available that fits the observations as well as make reasonable predictions of the future. In this case, consider the $m^{th}$ (*m*=1,2, .. .,*n*) largest observations of any continuous random variable. We ask: In how many cases, *x*, will the past $m^{th}$ observation be equaled or exceeded in *N* future trials? The rank *m* is counted from the top such that *m*=1 (*m*=*n*) stands for the largest (or smallest) observation. Therefore, the $m^{th}$ observation is the $m^{th}$ largest observation. The number of future years *N* for which a forecast is wanted need not be identical with the past sample size *n*. The number of cases *x*, called the number of exceedances, is a new statistical variable having a distribution $w(n,m,N,x)$ where *n*, *m*, *N* enter as parameters.

$$
w=\frac{\binom{n}{m}m\binom{N}{x}}{(N+n)\binom{N+n-1}{m+x-1}}
$$
Where the factor inside the brackets, $\binom{n}{k}$ is the binomial coefficient which represents the number of combinations that can be made from *n* items taken *k* at a time and where
$$
\binom{n}{k} = \frac{n!}{k!(n-k)!}
$$ 
  
In most cases the engineer is only concerned with the probability of exceeding some design threshold once in a certain number of future years since it only takes one exceedance for a possible failure. Therefore the example plot in Figure \@ref(fig:POE) shows the probability of a single exceedance for *m*= 1, 2, & 3 in 50 years based on a 30 year data history. Not surprisingly, a 1 in 30 year event has a maximum probability of occurring at year 30 and the second largest event is most likely at year 15. But using this method, it allows the designer to calculate the probability for an arbitrary number of years in the future. Figure \@ref(fig:POE) is the probability density of exceeding the $m^{th}$ largest event in Year $N$ based on $n$ years of observations. If the area under each curve was calculated it would equal to one, making it a true probability distribution not some ad-hoc rule-of-thumb. Unusually the interest to the engineer is not the probability of it happening in a given year but at what point in future years does the cumulative probability exceed some threshold. It serves to reason that the longer the wait since the previous maximum event the more likely that the event will be exceeded sooner rather than later going forward. This is shown in Figure \@ref(fig:cumprob) and is given by equation.

$$
F(N,n,m) = 1-\frac{n!(N+n-m)!}{(N+n)!(n-m)!}
$$
  
```{r POE_data}
n = 30
N = 1:50
x=1

#probability of exceeding the m_th highest observations, x times in N future trials based on n observations in past
w =function(N) ncol(combn(n,m))*m*ncol(combn(N,x))/((N + n)*ncol(combn(N+n-1,m+x-1)))

m <- 1
pw1 <- sapply(N, w)
pw1 <- unlist(pw1)
m <- 2
pw2 <- sapply(N, w)
pw2 <- unlist(pw2)
m <- 3
pw3 <- sapply(N, w)
pw3 <- unlist(pw3)

pw <- tibble(pw1, pw2, pw3) %>% 
  pivot_longer(cols = pw1:pw3)


# pw <- pivot_longer(pw,cols = everything())

pw$m <- rep(1:3,length(N))
pw$N <- rep(N, each = 3)

```

```{r POE, fig.cap="Probability of Exceedance", fig.width=6.5, fig.height=4.5}
pw %>%
  ggplot(aes(N, value)) +
  geom_line(aes(col = factor(m)),lwd=0.9) +
  theme_bw(14, "serif")+
  labs(
    title = "Probability of 1 Exceedance at N Future Years",
    subtitle = "Based on 30 Years Observed Data",
    y = "Probability",
    color = "Ranking"
  )
  

```

```{r cumprob, fig.cap="Cummulative Probability of Exceedance", fig.width=6.5, fig.height=4.5}

N <- rep(1:100,3)
n <- rep(c(10,20,30), each=100)
m <- 1

# For exceedance once of m ---------------------------------------------
a <- function(N,n){
  1-factorial(n)*factorial(N+n-m)/(factorial(N+n)*factorial(n-m))
  }

exced <- tibble(N,n) %>% 
  mutate(a = a(N,n))

exced %>%
  filter(a < 1) %>%
  ggplot(aes(N, a)) +
  geom_line(aes(col = factor(n)), lwd=0.9) +
  labs(
    title = "Cummulative Probability",
    subtitle = "of Exceeding Max Observation in N Future Years",
    x = "Future Years",
    y = "Probability of Exceedance",
    color = "Years of\nObservations"
  ) +
  theme_bw(14, "serif") +
  scale_x_continuous(breaks = scales::pretty_breaks())+
   scale_y_continuous(breaks = scales::pretty_breaks())

# For exceedance twice of m ---------------------------------------------

a1 <-
  1 - (factorial(n) * factorial(N + n - m) /
         (factorial(N + n) * factorial(n - m))) *
  ((N + n - m + N * m) / (N + n - m))




```

# Return Period {.unnumbered}

Related to the probability of exceedance is the return period. The return period is the average or estimated average time between events. This does not preclude the possibility of a 100 year event happening in two consecutive years (doesn't preclude but still very unlikely). The probability shown in Figure \@ref(fig:cumprob) of an event happening in a given year is not constant but dependent on the number of years since it last occurred. For instance, the probability of once per 100 year event in a given year is not 1/100 every year but is dependent on the number of years since that last occurred. If the actual return period is known such as a 100 year flood for example, then the *average* probability over all the years is $p=0.01$ but this is not that useful for any risk estimate as the probability changes each year since the last event. But if one wanted to know the probability of it occurring for the *first* time during year *t* then the geometric distribution shown in Equation will give that answer. 
$$
P_t=p(1-p)^{t-1}\quad t=1,2,... 
$$
  
Sometimes the issue is the probability of exceeding that threshold in any one year in a *t* year period. For that case it is the summation of the geometric probability from 1 to *t*. When considering the issue of a design capacity and wanting to know the probability of exceeding it at any time during the design life, then *t* is the desired design life.

$$
\sum_{t=1}^{t}p(1-p)^{t-1}
$$
  
# Intensity Prediction {.unnumbered}

Even though the probabilities of the extremes are small, the consequences are large. This is what is referred to as the, "Nuisance of Extreme Observations." Since the mean and dispersion of a distribution are considerably influenced by the extremes, a single observation can have significant impact on the interpretation of the observation. When an extreme is observed, it generates such questions such as, whether that observation comes from the same population as the rest, or is the observation is simply a bad reading or has the population undergone some sort of change. Criteria to interpret the validity of these readings was the motivation of the development of Extreme Value Theory (EVT). In the absence of EVT, engineers were forced to throw large (and costly) safety factors into designs to account for that uncertainty, leading to the phrase, "when in doubt build it stout.". While safety factors always need to be proportional to the magnitude of the uncertainty, it is not beneficial from a cost or risk standpoint to use excessive safety factors as a substitute for engineering.

# Prediction of Extreme Events {.unnumbered}

Black Swans occur without warning and have their origins in "unknown unknowns" therefore their time and place of happening are not predictable. Though the time and place are not predictable, there are methods that allow us to peek into the likelihood of an event of a certain size. This a well known realm of study in the area of extreme value analysis. If a dam was being designed, the engineer needs to have some sort of idea how large a flood it would have to contain, even though the historical data might not contain something of that magnitude. Things such as this were the impetus to develop methods that are capable of extrapolating probable future extremes from existing data.

To be able to accommodate the design of dams against historical floods or skyscrapers against worst-case wind loads, the theory of extreme distributions was developed. This separate branch of statistics deals the probability of rare and extreme events. As the name implies it is only concerned about the far right or left-tailed events. There are two ways of examining these types of events, one is the peak over threshold and the block maxima methods of extreme values. In the peak over threshold method, the designer is only concerned with the data points over a certain threshold regardless of when they happened. In the block maxima approach only the maximum value for each time period (block) is considered. An example of the block maximum method would be the consideration of the maximum rainfall recorded each year for 20 years.

```{r PHMSA GEVD}

inc_10m <- filter(incidents, CoF>9.9e6)
inc_cb <- filter(incidents, Year<2001)
fit_PHMSA <- fevd(incidents$CoF,threshold = 9.9e6)
q_phmsa <- qevd(c(0.975), fit_PHMSA$results$par[1],fit_PHMSA$results$par[2],fit_PHMSA$results$par[3])

p_SB <- pevd(max(incidents$CoF, na.rm=T), 
fit_PHMSA$results$par[1],
fit_PHMSA$results$par[2],
fit_PHMSA$results$par[3])

fit_carlsb <- fevd(inc_cb$CoF,threshold = 9.9e6)

```

# Extreme Value Distribution {.unnumbered}

The generalized extreme value distribution (GEVD) is made of three different but related distributions that combines the Gumbel, Fréchet and Weibull distributions (type I, II, & III respectively) into one family of distributions. There are three factors that govern the distribution, the location $\mu$, the scale $\sigma$ and the shape, $\xi$. The location is the mean of the distribution, the scale is a measure of the dispersion, similar to a standard deviation and the shape yields the type of extreme value distributions. When fitting the GEVD to data, the sign of the shape parameter $\xi$ will indicate which of the three models describes the random process that generated the data. The three cases are $\xi=0$, $\xi>0$ and $\xi<0$ for the Gumbel, Fréchet and Weibull distributions. The Weibull distribution of the GEV is often referred to as the "reverse" Weibull since it is bounded on the upper end and skewed left. The most common is the Gumbel which is used to model upper extremes that are bounded on the left and skewed right. The distribution density of the GEVD is defined by Equation . In Figure \@ref(fig:GEVplot) the probability density of these are plotted for three different cases discussed previously.
$$
f(s;\xi) =
\begin{cases}
(1+\xi s)^{\frac{-1}{\xi}-1}exp(-(1+\xi s)^{-1/\xi})\qquad &\xi\neq 0 \\
exp(-s)exp(-exp(-s)) \qquad &\xi=0
\end{cases}
$$

Where $s$ is the standardized variable

$$
s=\frac{x-\mu}{\sigma}
$$
  
```{r GEVplot, fig.cap="GEV Shapes", fig.asp=2/3}

colors <- c("-1/2" = "green", "0" = "red", "1/2" = "blue")

points <- data.frame(x=c(-2,2),y=c(0,0))

x <- data.frame(x=seq(-4,4,length.out = 301))

ggplot(x, aes(x)) +
  stat_function(
    fun = devd,
    args = list(
      loc = 0,
      scale = 1,
      shape = -1 / 2
    )  ,
    geom = "line",
    n = 301,
    xlim = c(-4, 2),
    aes(color = "-1/2")
  ) +
  stat_function(
    fun = devd,
    args = list(
      loc = 0,
      scale = 1,
      shape = 0
    ),
    geom = "line",
    n = 301,
    aes(color = "0")
  ) +
  stat_function(
    fun = devd,
    args = list(
      loc = 0,
      scale = 1,
      shape = 1 / 2
    )  ,
    geom = "line",
    n = 301,
    xlim = c(-2, 4),
    aes(color = "1/2")
  ) +
  geom_point(
    data = points,
    aes(x = x, y = y),
    col = c('blue', 'green'),
    shape = 8,
    size = 4
  ) +
  annotate(
    geom = "text",
    x = -2,
    y = 0.3,
    label = paste(expression(sigma), "==1", "~\n", expression(mu), "==0"),
    parse = TRUE,
    size = 5
  ) +
  theme_bw(14, "serif") +
  labs(
    title = "Generalized Extreme Value Densities",
    subtitle = "for various shape factors",
    y = "density",
    x = "x",
    color = "Shape"
  ) +
  scale_color_manual(values = colors)+
  theme(legend.position = c(0.85,0.75), 
        legend.box.background = element_rect(color = 'white'),
        legend.background = element_rect(color = 'black'),
        legend.text = element_text(size=12))

```

If a extreme value distribution is fit to the PHMSA incident data shown in Figure \@ref(fig:incident-plot) using a threshold of \$10 million, it would place San Bruno at almost the 95th percentile, implying that there is approximately a 5% chance of San Bruno being exceeded in the future. This is congruent with the quote from Emil Gumbel, "There will always be one (or more) value that will exceed all others." [^3]

[^3]: Gumbel, E. (1958) Statistics of Extremes, Columbia University Press, New York, USA

```{r phmsa-gev}
inc_fil <- incidents %>%
  filter(CoF>1e7, System=="GT (Gas Transmission)")

fevd_cof <- fevd(inc_fil$CoF) #fit evd to CoF data

#histogram with evd distribution curve over it####
inc_fil %>%
ggplot(aes(CoF)) +
  geom_histogram(fill = 'firebrick2',
                 col = 'black',
                 aes(y = ..density..),
                 bins = 50) +
  theme_bw(16, "serif") +
  theme(
    plot.margin = margin(0.5, 0.5, 0.5, 0.5, "cm"),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  stat_function(
    fun = dgev,
    args = list(
      fevd_cof$result$par[[1]],
      fevd_cof$result$par[[2]],
      fevd_cof$result$par[[3]]
    ),
    col = 'blue',
    lwd = 1.2,
    n = 401
  ) +
  labs(
    x = "Consequence of Failure ($)",
    title = "Extreme Value Distribution",
    subtitle = "PHMSA Incident Data (1986-2021)",
    y = NULL,
    caption = "Filtered for incidents > $10MM"
  ) +
  xlim(1e7, 1e8)

```
 
# Conclusion {.unnumbered}

Even though there is theoretical backing to predicting the probable magnitude of extremes, it still does not predict where or precisely when they will occur. Because of this, it is important to have what can be termed as defense in depth. Meaning that there are enough redundant layers of protection between the ever-present hazard and an actual accident, that any gap in a single layer does not give a pathway between the hazard and a potentially catastrophic accident. These defensive barriers take the form of procedures, materials, maintenance, inspection and pipeline patrols to name a few. The more redundant and diverse the barriers, the less likely a Black Swan has any viability in your system. Above all else, follow the Noah Rule: *Predicting rain does not count, building arks does.* As risk engineers we are called to not just be rain predictors but ark builders as well.
