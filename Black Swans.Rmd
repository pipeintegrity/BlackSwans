---
title: "The Bird of Catastrophy"  
author: "CERM"  
date: "`r format(Sys.time(), '%d %B, %Y')`" 

output: bookdown::word_document2
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

## Black Swans
The term "Black Swan" originates from the belief that all swans were white because those were the only ones observed by westerners.  This belief continued until 1697 when black swans were found in Australia, rewriting accepted science in one swipe.  The term becomes a metaphor for unprecedented and unexpected events and eventually the title of a best selling book by Nasim Taleb.  
  
There are several characteristics that all Black Swans have in common. The first is that it comes as a surprise to the observer.  It is an outlier outside the realm of regular expectations because nothing in the past points to its existence.  Typically this is because the time frame of which data is available is too short to contain one of these Black Swans due to their rarity.  
  
The second factor that all Black Swans have in common is that they are rationalized after the fact, making them "predictable" in retrospect in the observers mind.  Explanations are created to convince the observer that if they had only looked, the evidence was there the whole time.  Even though there is nothing in the past that points to it.
  
The third characteristic is that it has extreme impact beyond anything ever observed previously.  The consequences can be thousands of times a "typical" incident.  The three ingredients to a Black Swan are the unpredictability, consequences not seen in any historical record and rationalization after the fact. 
  
## How bad can bad be
In any risk analysis the question always at hand is, how bad can it be?  The response is reflexively, how bad has it been in the past. The problem with that approach is that Black Swans are a rare bird and your window of observations might be too short to have seeen one in your data set. In addition, when people are asked about the magnitude of risk for something in which hard data is sparse or non-existent they will base their estimates based on personal experience which is most likely skewed toward the lower quartile. People are more likely to give credibility to stories than they are abstract statistics since it is easier to mentally process.
  
## Black Swans and Statistics  
Most of the traditional statistics serve the user well if the phenomena of interest never ventures far from the average. It can be seen in the following examples that catastrophic risk does not adhere to such norms. Some distributions are so common in many everyday applications where the effects are additive and the deviations from the mean are small.  The classic example of this is height of people.  The average height of men is about 5' 10" with a standard deviation of 4".  It would be very rare to come across someone that is more than 2 or 3 standard deviations from that mean.  We don't have giants or Lilliputians walking around!  But in the field of risk, a Black Swan can be a hundred or a thousand times the mean. This is because the variables that drive risk are multiplicative rather than additive therefore the results have the ability to increase exponentially. That is why it is very dangerous to make assumptions that make something computationally convenient (e.g. normally distributed or taking an average) because Black Swans live in the wild, out on the tails of the distribution where traditional statistics would predict the likelihood of something of that magnitude is essentially zero.  The first characteristic of Black Swans is that they are a rare bird, your period of time that you have data for might be too short to have ever seen one.  If your entire data set contains nothing but ordinary "White Swans" it does not prove the lack of existence of Black Swans but a single Black Swan is sufficient to disprove that theory of its non-existence and one Black Swan can put a company out of business in one swipe.

## The Perils of the Normal Distribution  
The following simulation shows why the normal distribution is misleading in the area of rare risks. The simulation makes 1,000 random draws from a standard normal distribution (zero mean and unit standard deviation) and repeats this process 1,000 times (1 million samples total). With each iteration it calculates the mean and maximum value, plotted as blue and red markers respectively. The bottom plot shows that the density of the maximum values converge to a Gumbel distribution known as a Type I extreme value distribution which will be discussed later in this paper. But more importantly it can be seen that even though the samples are drawn at random, the average of the samples never get far from the mean (about $\pm$ 0.1 standard deviations) of the population and the extremes *start* at about 3 standard deviations from the mean. If the sample data was examined it would be seen that only about one-tenth of one percent (0.1%) ever exceeded 3 standard deviations. The implications of this is that if an engineer is working with a set of limited observations and assumes the entire range of data is normally distributed, the magnitude and frequency of an extreme event will be grossly underestimated and the observation data might not even contain anything beyond two standard deviations much less three. But this also demonstrates that extreme events follow their own distribution separate from the rest of the population that can be exploited as will be seen later in this paper.

```{r gumbel, fig.height=5, fig.width=6.5, dpi=300, cache=TRUE}
library(locfit)
library(logspline)
library(patchwork)
library(tidyverse)


#Tidy verse way -----------------------------------------------------


it <- 1:1000
n <- rep(0,max(it))

df2 <- bind_cols(n=n, it =it)

df <- map(n, ~ rnorm(n = 1000, mean = .x, 1)) %>%
  tibble() %>%
  bind_cols(df2) %>%
  unnest(cols = 1) %>%
  rename(value = ".")

max <- df %>%
  group_by(it) %>%
  summarise(max=max(value),
            ave=mean(value),.groups = "drop")


norm_dist <- df %>%
  ggplot(aes(value)) +
  geom_density(col = 'grey',
               aes(group = it),
               alpha = 0.005, 
               lwd=0.15) +
  geom_point(data = max,
             aes(x = max, y = 0),
             col = 'red',
             shape = 2) +
  geom_point(data = max,
             aes(x = ave, y = 0),
             col = 'blue',
             shape = 2, alpha=0.1) +
  theme_minimal(15)+
  labs(x="Normal Distribution", y=NULL)


gumbel <- max %>%
  # filter(max>2.5) %>%
  ggplot(aes(max))+
  stat_density(geom="line",col='red',lwd=1, alpha=0.5)+
  theme_minimal(15)+
  coord_cartesian(xlim=c(-5,5))+
  labs(y =NULL,
       x = "Extreme Value Distribution (Gumbel)")


norm_dist/gumbel
# ggarrange(norm_dist,gumbel,nrow = 2)


```

It can be said that statistics and probability is a statement about the future based on observations in the past. But risk is not a constant thing. People constantly alter their behavior based on observed or perceived dangers therefor altering future outcomes. Making past observations possibly moot if it based on an assumed pattern of behavior.  Donald Rumsfeld, former  Secretary of Defense talked about the three types of "knowns".  The first is the "known knowns", things that you know for certain.  The second is the "known unknowns", these are the things that you know you don't know but since you are aware of them they could be acquired through time, effort or other resources.  The third and most consequential is the "unknown unknowns", these are the things that you don't know you don't know - you are completely unaware of them in any way. This is the origins of Black Swans.  For if the factors that could lead to a Black Swan were known, the end user would have made adjustments to their behavior to account for them.  But instead the average person careens ahead attempting to mitigate and prevent the directly perceived more common risk factors that are seen on a regular basis, in the meantime just out of sight could be an event that end their very existence. This is known as availability bias, the more recent events are given more credibility even if evidence to the contrary exists.  An example would be people that they say they would never swim in the ocean after hearing of a shark attack, even though it is an extremely rare event when compared to things like being attacked by dogs. Sharks are certainly more deadly but you are far more likely to encounter a dog than a shark in your daily life.
  
## You Make the Call
There used to be a commercial that played during NFL games where they would show footage of an unusual play from an NFL game and then ask the audience, "You make the call". In this exercise you are the risk engineer and you are handed the following data and are asked what is the worst case scenario for loss ratios for the next year. What would you say? Hopefully you know enough that the average is too optimistic and the worst data point on record is probably too pessimistic. So you assume the data is normally distributed and you know that in a normal distribution 95% of the population lies approximately 2 standard deviations from the mean.  So you make your worst case prediction a ratio of 67. After all, the entire 23 years of data is less than 50 with the exception of one outlier year and 75% of the data is 12 or less.  Seems like a solid prediction to the engineer.

```{r insurance losses, fig.cap="Figure 2: Insurance Losses"}

eq_insurance <- read.csv("C:/Users/Joel/OneDrive/Analysis/EVT/earthquake_loss.csv")

ggplot(eq_insurance[-24], 
       aes(year, loss_ratio))+
  geom_bar(stat = "identity", 
           fill='steelblue2',
           col='black')+
  theme_bw(14, "serif")+
  scale_y_continuous(breaks=seq(0,140,by=20))+
  labs(title = "Earthquake Insurance Loss Ratios by Year",
       subtitle = "1971 - 1993", 
       x="Year", 
       y="Loss Ratio")
#ggplot(insurance[-24], aes(loss_ratio))+geom_histogram( fill='steelblue2',col='black')+theme_bw()+scale_y_continuous(breaks = seq(0,10, by=2))
```
  
The conclusion of this is that in 1994 the Northridge Earthquake hit with a loss ratio of 2273, almost 700 times the median loss ratio of the previous twenty-three years.  Even if someone was to assume worst case was the mean plus three standard deviations, which under a normal distribution covers 99.7% of the data they would have still been off by a factor of more than 20.  It can be seen from this example that catastrophic risk does not follow statistical norms. But in the field of risk, a Black Swan can be a hundred or a thousand times the mean.  This means that the results have the ability to increase exponentially and are not bound by the artificial constraints of a statistical distribution.
  
This also means that even analysis methods like a Monte Carlo even though they account for stochastic nature of different variables, are better at predicting average behavior of something than the extremes. This is because the sampling distributions used are more prone to model the "typical" rather than the extreme consequently leading to "typical" results.

## Catastrophic Risk  
While history is not always a good predictor of the future, that is all is available to mine for data. Caveat: the shorter your window of observation the more likely it will be exceeded and in a shorter amount of time.  In the case of pipeline risk, PHMSA publishes incident data for jurisdictional pipelines.  An incident is an accidental release from a pipeline that includes any of the following: Greater than \$50,000 in property damage, an injury or fatality.  Then if you were to take that data and scale the injuries and fatalities to dollar values using an appropriate multiplier to arrive at a total consequence of failure (CoF), the data would have a histogram as shown in Figure 1.  Note: the range of data for the histogram extends out to $10^9$ on the logarithmic axis but for the purposes of scale it is truncated at $10^8$.
  
```{r import data}

incidents <- read.csv("all_combine.csv") %>%
  filter(!is.na(Year)) %>%
  mutate(inflation_adjstd = TOTAL_COST_IN84 * 2.49,
         CoF = inflation_adjstd + INJURE * 1 / 3 * 10e6 + FATAL * 10e6) %>%
  filter(!is.na(CoF))
#Per CPI calculator adjusted 1984 to 2019
```

```{r incident-plot, fig.cap="CoF Data Density Curve"}
incidents %>%
  ggplot(aes(CoF)) +
  geom_histogram(fill = '#FF3030',
                 col = 'black',
                 alpha = 0.8) +
  geom_rug(col = 'blue') +
  theme_bw() +
  xlim(0, 1e8) +
  ylim(0, 60) +
  labs(title = "Consequence of Failure",
       subtitle = "PHMSA Gas Transmission Incidents 1986 - 2019",
       x = "CoF (log Scale)")

```

In the field of risk and in most naturally occurring phenomena, the majority of the data will almost always lie in a predictable range near the lower end of the scale but the incidents that should keep one up at night are the ones to the far right which are much rarer but the consequences can be larger than the anything seen in historical data.  In the CoF data in Figure 1 the median incident is around \$170,000 and the maximum is over \$1 billion a difference of over 6,000 times.  This is common in any application of risk, not just pipeline risk. There will be a plethora of data at the lower end of the scale with outliers extending out several magnitudes of order.  
  
```{r incident-gt5M-plot}
incidents %>%
  filter(CoF >= 5e7) %>%
  ggplot(aes(CoF)) +
  geom_histogram(fill = '#FF3030',
                 col = 'black',
                 alpha = 0.8) +
  geom_rug(col = 'blue') +
  theme_bw() +
  labs(title = "Consequence of Failure",
       subtitle = "PHMSA Gas Transmission Incidents 1986 - 2019",
       x = "CoF (log Scale)")
```
  
If a normal distribution was fit to the incident CoF data it would have the appearance of being a good fit through the middle of the curve with some outliers at the higher end.  But if the tails area was magnified, you would see that there are far more of these extreme events than would be expected based on standard statistics. This why the most distributions are only a suitable model for such events that phenomena is based on sums and averages but a really poor model for Black Swans where the effects are multiple magnitudes of order from the mean. Traditional statistics would predict the likelihood of something of the magnitude of these consequences to be effectively zero, yet there are several examples)in the 33 years of data departing from this. This implies that there is something else outside of traditional statistics that describes these extremes. In fact the data count departs from the predicted count by a factor of 10 at the higher CoF values. The seemingly goodness of fit in the more prevalent lower end will often lead people to disregard the extreme events as so remote as not to be given serious consideration, much to their peril.  

## Extremes  
The aim of statistical theory of extreme values is to explain the observed extremes arising in the samples of given sizes, or for a given period and to forecast extremes that may be expected to occur within a certain sample size, time, area, etc. There are two types of predictions that are can be made, one is the intensity of an event and the other is the frequency.  The frequency can be based on a set of observations or on a known period of return.  The founders of the calculus of probabilities were too occupied with the general behavior of statistical masses to be interested in the extremes. Nineteenth century mathematician Fourier stated that, for the normal distribution, the probability of an absolute deviation to exceed $3\sqrt 2$ times the standard deviation is about 1 in 50,000, and could therefore be neglected. From this small probability, the erroneous conclusion was drawn that about three times the standard deviation should be considered as the maximum for any statistical variable. The idea that three times the standard deviation should be considered as maximum-irrespective of the number of observations and of the distribution still prevails among most "practical" people. However, the fallacy of this rule is obvious. If the variable being studied is unlimited then the largest value is unlimited as well.  Emil Gumbel who did a considerable amount of the pioneering work on extreme value theory is quoted, "There will always be one (or more) value that will exceed all others." and the President's Water Commission () stated that,"However big floods get, there will always be a bigger one coming; so says one theory of extremes, and experience suggests it is true."

Fortunately this phenomena has been studied at length in areas such as floods and insurance risks. The first problem of the extremes to be discussed is when the available data is set of observations over a fixed period of time and the desire is to know what the likelihood of exceeding the maximum observed value in some amount of time in the future. An example of this would be a data set of 50 years of rainfall and the engineer wants to know what the likelihood of exceeding the maximum observed amount in the next 30 years.

## The Law of Rare Exceedances 
Often how frequent an extreme event is likely to occur on average is not known ahead of time and the designer only has a set of observations over some time period. The shorter the observation period the more likely the largest event observed will be exceeded in future observations and in a shorter amount of time. If the sample size is increased,the largest value observed will likewise increase. In this case, consider the *m*th (*m*=1,2, .. .,*n*) observations of any continuous random variable. We ask: In how many cases, *x*, will the past *m*th observation be equaled or exceeded in *N* future trials? The rank *m* is here counted from the top such that *m*=1 (*m*=*n*) stands for the largest (or smallest) observation. Therefore, the *m*th observation is the *m*th largest observation. The sample size *N* for which a forecast is wanted need not be identical with the past sample size *n*. The number of cases *x*, called the number of exceedances, is a new statistical variable having a distribution w(n,m,N,x) where *n*, *m*, *N* enter as parameters.

\begin{equation}
w=\frac{\binom{n}{m}m\binom{N}{x}}{(N+n)\binom{N+n-1}{m+x-1}}
(\#eq:POE)
\end{equation}

  
Where the factor inside the brackets, $\binom{n}{k}$ is the binomial coefficient which represents the number of combinations that can be made from *n* items taken *k* at a time and where

\begin{equation}
\binom{n}{k} = \frac{n!}{k!(n-k)!}
(\#eq:binom)
\end{equation}

In most cases the engineer is only concerned with the likelihood of exceeding some design threshold once in a certain number of future years since it only takes one exceedance for a possible failure.  Therefore this example plot shows the probability of a single exceedance for *m*= 1, 2, 3 in 50 years based on a 30 year data history.  Not surprisingly, a $\frac{1}{30}$ year event has a maximum probability of occurring at year 30 and the second largest event is most likely at year 15.  But using this method, it now allows the designer to compute the likelihood for an arbitrary number of years in the future.  
  
```{r POE, fig.cap="Probability of Exceedance"}
n = 30
N = seq(1,50,by=1)
x=1

#probability of exceeding the m_th highest observations, x times in N future trials based on n observations in past
w =function(N) ncol(combn(n,m))*m*ncol(combn(N,x))/((N + n)*ncol(combn(N+n-1,m+x-1)))

m <- 1
pw1 <- sapply(N, w)
pw1 <- unlist(pw1)
m <- 2
pw2 <- sapply(N, w)
pw2 <- unlist(pw2)
m <- 3
pw3 <- sapply(N, w)
pw3 <- unlist(pw3)

pw <- data.frame(cbind(pw1, pw2, pw3))

pw <- reshape2::melt(pw)
# pw <- pivot_longer(pw,cols = everything())

pw$m <- c(rep(1,length(N)), rep(2,length(N)), rep(3,length(N)))
pw$N <- rep(N,3)


pw %>%
  ggplot(aes(N, value)) +
  geom_line(aes(col = factor(m)),
            lwd = 1) +
  theme_bw(14, "serif") +
  labs(
    title = "Probability of One Exceedance in N Future Years",
    subtitle = "Based on 30 Years Observed Data",
    y = "Probability",
    color = "Event Ranking"
  )

```
  
## Return Period  
Related to the probability of exceedance is the return period.  The return period is the average or estimated average time between events. This does not preclude the possibility of a 100 year event happening in two consecutive years (doesn't preclude but still very unlikely). The probability shown in Figure \@ref(fig:POE) of an event happening in a given year is not constant but dependent on the number of years since it occurred.  For instance the probability of a 1 in a 100 year event in a given year is not $\frac{1}{100}$ every year but is dependent on the number of years since that last occurred. If the actual return period is known such as a 100 year flood example then the probability for in any one year is obviously $p=0.01$.  But if it was desired to know the probability of it occurring for the *first* time during year *t* then the geometric distribution will give that answer. When considering the issue of a design capacity and wanting to know the probability of exceeding it during the design life, then *t* is the desired design life.

\begin{equation}
P_t=p(1-p)^{t-1}\quad t=1,2,... (\#eq:POE)
\end{equation}

Sometimes the issue is the likelihood of exceeding that threshold in any one year in a *t* year period. For that case it is the summation of the geometric probability from 1 to *t*.

\begin{equation}
\sum_{t=1}^{t}p(1-p)^{t-1}(#eq:POE_Sum)
\end{equation}

## Intensity Prediction  
Even though the probabilities are small, the consequences can be significant.  This is what Gumbel referred to as the," Nuisance of Extreme Observations." Since the mean and dispersion of a distribution are considerably influenced by the extremes, a single observation can have significant impact on the interpretation of the observation. It begs questions such as, whether that observation comes from the same population as the rest, is the observation is simply a bad reading due to human or instrument error or has the population undergone some sort of change? Criteria to interpret the validity of these readings was the motivation of the development of Extreme Value Theory (EVT). In the absence of EVT, engineers were forced to throw large (and costly) safety factors into designs to account for that uncertainty. 
  
## Prediction of Extreme Events
Black Swans occur without warning and have their origins in "unknown unknowns" therefore their time and place of happening are not predictable. Though the time and place are not predictable, there are methods that allow us to peek into the likelihood of an event of a certain size. This a well known realm of study in the area of floods and extremes of weather. If a dam was being designed the engineer needs to have some sort of idea how large a flood it would have to contain, even though the historical data might not contain something of that magnitude. 
  
To be able to accommodate the design of things such as dams against historical floods or skyscrapers against wind loads the theory of extreme distributions was developed.  This separate branch of statistics deals the probability of rare and extreme events. As the name implies it is only concerned about the far right or left-tailed events. There are two ways of examining these types fo events with the GEVD, one is the peak over threshold and the block maxima. In the peak over threshold method the designer is only concerned with the data points over a certain threshold. In the block maxima approach only the maximum value for each time period is considered, for example the maximum rainfall recorded each year for 20 years. How does this compare to PHMSA incident data?  
  
```{r PHMSA GEVD}
library(extRemes)

inc_10m <- filter(incidents, CoF>1e7)
fit_PHMSA <- fevd(inc_10m$CoF)
q_phmsa <- qevd(c(0.975), fit_PHMSA$results$par[1],fit_PHMSA$results$par[2],fit_PHMSA$results$par[3])/1e6
p_SB <- pevd(max(incidents$CoF), fit_PHMSA$results$par[1],fit_PHMSA$results$par[2],fit_PHMSA$results$par[3])
```
  
## Extreme Value Distribution
  
The generalized extreme value distribution (GEV) is made of three different but related distributions that combines the Gumbel, Fréchet and Weibull distributions (type I, II, & III respectively) into one family of distributions. There are three factors that govern the shape of the distribution the location,$\mu$, the scale $\sigma$ and the shape, $\xi$.  The location is the mean of the distribution, the scale is a measure of the dispersion, similar to a standard deviation and the shape yields the type of extreme value distributions. When fitting the GEV to data, the sign of the shape parameter $\xi$ will indicate which of the three models describes the random process that generated the data. The three cases are $\xi=0$, $\xi>0$ and $\xi<0$ for the Gumbel, Fréchet and Weibull distributions.  The Weibull distribution of the GEV is often referred to as the "reverse" Weibull since it is bounded on the upper end and skewed left.  The most common is the Gumbel which is used to model upper extremes that are bounded on the left and skewed right. The distribution density of the GEV is defined by the following equation.  Figure 4 is the density of these are plotted for three different cases for the distribution discussed previously.
  
$$f(s;\xi)=\bigg\{\begin{align*} 
(1+\xi s)^{\frac{-1}{\xi}-1}exp(-(1+\xi s)^{-1/\xi})\qquad \xi\neq 0 \\ 
exp(-s)exp(-exp(-s)) \qquad \xi=0
\end{align*}$$
  
Where $s$ is the standardized variable
  
$$s=\frac{x-\mu}{\sigma}$$
  
```{r GEV plot, fig.cap="Figure 4"}
library(extRemes)
points <- data.frame(x=c(-2,2),y=c(0,0))
x <- data.frame(x=seq(-4,4,length.out = 301))
ggplot(x, aes(x)) +
  stat_function(
    fun = devd, args =list(loc=0,scale=1,shape=-1/2)  ,
    geom = "line",
    col = "green",
    n=301, xlim = c(-4,2),lwd=0.9
  )+
  stat_function(
    fun = devd, args =list(loc=0,scale=1,shape=0)  ,
    geom = "line",
    col = "red",
    n=301,lwd=0.9
  )+
  stat_function(
    fun = devd, args =list(loc=0,scale=1,shape=1/2)  ,
    geom = "line",
    col = "blue",
    n=301, xlim = c(-2,4),lwd=0.9
  )+
  geom_point(data=points,aes(x=x,y=y),col=c('blue','green'), shape=8, size=4)+
  annotate(geom="text", x=-2,y=0.3, label=paste(expression(sigma),"==1","~\n ",expression(mu),"==0"), parse=TRUE, size=5)+
  theme_bw()+
  labs(title = "Generalized Extreme Value Densities", subtitle = "for various shape factors", y= "density",x="x", caption = "shape = -1/2, 0, +1/2 (green, red, blue)")

```
  
If a extreme value distribution is fit to the PHMSA, incident data using a threshold of \$10 million, the far right tail of the 95% prediction interval (97.5 percentile) would be at about \$900 million which is within 11% of the CoF of San Bruno. Using the same model, it would place San Bruno at almost the 98 percentile, implying that there is a slightly greater than 2% chance of San Bruno being exceeded in the future. This is congruent with the quote from Emil Gumbel who did a lot of the pioneering work on extreme value theory, "There will always be one (or more) value that will exceed all others." and the President's Water Commission in 1932 stated that,"However big floods get, there will always be a bigger one coming; so says one theory of extremes, and experience suggests it is true."
  
## Conclusion  
Even though there is theoretical backing to predicting the likely magnitude of extremes it still does not predict where or when they will occur. Because of this, it is important to have what can be termed as defense in depth. Meaning that there are enough redundant layers of protection between the ever-present hazard and an actual accident that any gap in a single layer does not give a pathway between the hazard and a potentially catastrophic accident.  These defensive barriers take the form of procedures, materials, maintenance and pipeline patrols to name a few.  The more redundant and diverse the barriers the less likely a Black Swan has any viability in your system. Above all else follow the Noah Rule: Predicting rain does not count, building arks does. As risk engineers we are called to not just be rain predictors but ark builders as well.
